Comprehensive Pipelines for Structural and Copy Number Variation Analysis from Whole Genome Sequencing DataPart I: Foundational Principles and Pipeline ArchitectureSection 1: The Landscape of Structural Variation in WGS Data1.1 Defining the Spectrum of Genomic VariationThe human genome is not a static reference sequence but a dynamic entity characterized by variation across individuals and populations. This variation exists on a continuum of scales. At the smallest scale are Single Nucleotide Variants (SNVs) and small insertions/deletions (indels), typically less than 50 base pairs (bp) in size. While numerous, the total number of base pairs affected by these small variants is less than that affected by larger-scale rearrangements.1 These larger events, collectively known as Structural Variation (SV), are formally defined as genomic alterations involving segments of DNA that are approximately 50 bp or larger.1 SVs represent a major source of genetic diversity and are a primary driver of genome evolution.1SVs encompass a diverse array of rearrangement types, which can be broadly categorized into two classes based on their effect on the total DNA content of the genome.3Unbalanced SVs (Copy Number Variations - CNVs): These variants alter the dosage of genetic material. They are the most common type of SV and include deletions (loss of a DNA segment) and duplications (gain of a DNA segment).3 A CNV present at a frequency of greater than 1% in a population is often referred to as a copy number polymorphism (CNP).2 The Database of Genomic Variants (DGV) has annotated a significant portion of the human genome as copy number variable, underscoring its prevalence.4Balanced SVs: These variants do not result in a net gain or loss of DNA but rather a change in its organization or orientation. This class includes inversions (a segment of DNA is excised, flipped 180 degrees, and reinserted) and translocations (a segment of DNA moves from one genomic location to another, which can be on the same or a different chromosome).1It is crucial to understand that CNV is a functional consequence (change in copy number), while SV is a mechanistic description of the rearrangement event. Therefore, CNV is considered a subset of SV.1 This distinction is not merely semantic; it has profound implications for detection strategies, as methodologies sensitive to changes in DNA dosage may be completely blind to balanced rearrangements, and vice versa.1.2 Biological and Clinical SignificanceStructural variations are not merely neutral markers of genetic diversity; they are potent architects of phenotype and disease. By altering gene dosage, disrupting gene structures, or modifying regulatory landscapes, SVs can have profound biological consequences.3 They affect more base pairs in a typical human genome than all SNVs combined and are known to have more drastic effects on gene expression.1The clinical relevance of CNVs is well-established. A host of well-characterized genomic disorders are caused by recurrent microdeletions or microduplications, including DiGeorge/velocardiofacial syndrome, Williams-Beuren syndrome, and Prader-Willi syndrome.4 As genotype-phenotype correlations have become more refined through large-scale sequencing projects, the list of SV-associated diseases has expanded to include complex neurodevelopmental disorders such as Coffin-Siris syndrome and early infantile epileptic encephalopathy.2 Beyond rare Mendelian diseases, SVs are implicated in complex traits and genome instability, which is a hallmark of many cancers.1The impact of SVs extends beyond human health. In agriculture, SVs can confer tolerance to adverse environmental conditions and positively influence crop yield and quality, making them a key target in botanical research and breeding programs.2 This wide-ranging impact underscores the critical need for accurate and comprehensive methods for their detection and characterization from genomic data.1.3 Detection Signatures in Short-Read WGS DataThe primary challenge in SV detection from short-read Whole Genome Sequencing (WGS) data stems from the fact that the reads (typically 150 bp) are much shorter than the variants being sought. Consequently, SVs cannot be observed directly. Instead, their presence must be inferred from the patterns and signatures they leave in the alignment of these short reads against a reference genome. There are four principal types of evidence used by modern SV callers, each with distinct strengths and weaknesses.Paired-End (PE) Mapping: Standard Illumina sequencing generates paired-end reads, where two reads are sequenced from opposite ends of a single DNA fragment of a known approximate size (the "insert size"). When these read pairs are mapped to the reference genome, their distance and orientation are expected to fall within a predictable distribution. SVs disrupt this expectation. For example, a deletion in the sample genome will cause the mapped read pairs to appear closer together than expected, while an insertion will cause them to map farther apart. An inversion will cause one read in a pair to map with an inverted orientation.1 This signature is powerful for detecting the presence of an SV but often provides imprecise breakpoint coordinates.Split-Read (SR) Alignments: A split-read occurs when a single sequencing read spans an SV breakpoint. When aligned to the reference, one portion of the read will map to one location, while the other portion maps to a disjoint location (or is unmapped). The point of the "split" in the read alignment provides direct, base-pair resolution evidence of the SV breakpoint.1 This signature is the gold standard for precise breakpoint localization but is only effective for reads that happen to fall directly across the junction of the rearrangement.Read-Depth (RD) Analysis: The depth of coverage at any given genomic locus is expected to be relatively uniform in a diploid genome. CNVs disrupt this uniformity. A heterozygous deletion will result in a ~50% reduction in read depth, while a duplication will cause an increase. By analyzing coverage in windows across the genome and comparing it to a baseline (either a genome-wide average or a panel of control samples), one can infer copy number states.1 This method is highly effective for detecting large deletions and duplications (CNVs) but cannot detect balanced SVs like inversions or translocations, as these events do not alter the read depth.3 Furthermore, its resolution is limited by the size of the window used for analysis.7De Novo Assembly (AS): In theory, the most accurate way to identify all forms of variation would be to assemble the short reads into long contiguous sequences (contigs) de novo (without a reference) and then align these contigs to the reference genome. This approach can resolve complex SVs that are intractable with other methods.1 However, de novo assembly from short reads is computationally intensive and often results in fragmented assemblies, especially in repetitive regions of the genome where SVs are common. For this reason, pure assembly-based approaches are less frequently used for large-scale SV discovery from short-read data compared to alignment-based methods.The existence of these orthogonal detection signatures is the central pillar upon which a comprehensive SV analysis strategy must be built. No single method can reliably detect all types of SVs. Read-depth methods are essential for CNVs but miss balanced events. Paired-end and split-read methods are essential for breakpoint detection and balanced events but can struggle with quantifying copy number in complex regions. This necessitates an "ensemble" approach, where multiple algorithms leveraging different signatures are combined to achieve the most complete picture of structural variation in a genome.Section 2: Architectural Blueprint for SV/CNV Analysis2.1 The Multi-Stage Analysis FrameworkBased on established best practices and the fundamental properties of WGS data, a robust pipeline for SV and CNV analysis can be structured into six core stages. This modular framework ensures that data processing is logical, traceable, and that quality control is integrated at critical junctures. The overall workflow proceeds as follows:Data Ingestion & Quality Assurance: Begins with raw sequencing reads (FASTQ format). This stage involves assessing the initial quality of the data and performing cleaning steps such as adapter trimming and quality filtering to remove technical artifacts.Alignment & Preparation: The cleaned reads are aligned to a reference genome (e.g., GRCh38) to produce a BAM file. This file is then sorted, indexed, and processed to mark duplicate reads, making it ready for variant analysis.Structural Variant (SV) Calling: An ensemble of callers that primarily use paired-end and split-read signatures are run on the prepared BAM file. This stage is optimized for detecting breakpoints with high precision and identifying all classes of SVs, including balanced events.Copy Number Variation (CNV) Calling: A set of complementary callers that use the read-depth signature are run. This stage is specifically designed to detect unbalanced events (deletions and duplications) by analyzing coverage fluctuations.Integration & Refinement: The outputs from the multiple SV callers are merged into a single, unified callset. Both the SV and CNV callsets are then subjected to rigorous filtering to remove likely false positives and enrich for high-confidence variants.Annotation: The final, high-confidence SV and CNV callsets are annotated with a wealth of functional information. This includes identifying which genes are affected, predicting the molecular consequences, and cross-referencing against databases of known pathogenic variants and population frequencies to aid in interpretation and prioritization.This multi-stage design provides a clear and logical progression from raw data to interpretable, annotated variants, forming the backbone of both pipelines presented in this report.2.2 The Ensemble Calling StrategyA central tenet of this report's methodology is the use of an ensemble calling strategy. The limitations of short-read WGS data make it exceedingly difficult for any single algorithm to capture the full spectrum of SVs with high sensitivity and specificity.7 Different tools have different strengths, are optimized for different SV types and size ranges, and leverage different underlying statistical models and detection signatures. For instance, GATK-SV, a widely recognized best-practice pipeline, itself integrates calls from multiple algorithms including Manta and GATK-gCNV to produce its final output.8Therefore, to construct a truly comprehensive pipeline, this report advocates for a dual-pronged approach within the variant calling stages:For SVs (Breakpoint-centric): We will combine the outputs of Manta, Delly, and Lumpy. These tools are all state-of-the-art callers that primarily rely on paired-end and split-read evidence.9 By integrating their results, we can leverage their complementary algorithms to maximize the discovery of deletions, duplications, insertions, inversions, and translocations, while also gaining confidence in calls supported by multiple tools.For CNVs (Dosage-centric): We will use two distinct read-depth based callers: CNVnator and GATK-gCNV. CNVnator employs a mean-shift algorithm on binned read-depth data and is effective for single-sample analysis.12 GATK-gCNV uses a more sophisticated Bayesian model that is particularly powerful when analyzing cohorts of samples, as it can learn and account for systematic coverage biases.13This ensemble strategy is not about redundancy; it is a deliberate design choice to address the inherent complexities of SV detection. By combining evidence from orthogonal methods, the resulting callset is more comprehensive and robust than what any single tool could produce alone.2.3 Pipeline Modalities: Bash vs. SnakemakeTo meet the user's request for both a direct script and a scalable workflow, this report will detail the implementation of the six-stage architecture in two distinct modalities.The Sequential Bash Pipeline (.sh): This implementation will be presented as a single, self-contained bash script. It will execute each stage of the analysis in a linear, sequential fashion. This format is highly valuable for pedagogical purposes, as it clearly illustrates the command-line syntax for each tool and the flow of data between steps. It is well-suited for analyzing a single sample or for environments where the installation of a workflow management system is not feasible.The Snakemake Pipeline: This implementation will use the Snakemake workflow management system, a powerful tool for creating reproducible, scalable, and portable bioinformatics pipelines.14 The analysis will be defined as a series of interconnected "rules" in a Snakefile. Snakemake automatically determines the dependencies between rules, enabling massive parallelization on multi-core machines or compute clusters. Crucially, it integrates with the Conda package manager to handle software dependencies automatically, ensuring that the pipeline can be run reproducibly by anyone, on any system, at any time.14 This modality represents the production-grade, best-practice approach for analyzing multiple samples or for any research intended for publication.Parts II and III of this report will provide the complete, documented code and detailed explanations for each of these two implementations.Part II: The Sequential Bash Pipeline: A Step-by-Step ImplementationThis part details the construction of a comprehensive SV and CNV analysis pipeline as a sequential bash script. This script is designed for clarity and direct execution, serving as both a functional tool for single-sample analysis and a pedagogical guide to the underlying commands and data flow.Section 3: Stage 1 - Data Ingestion and Quality AssuranceThe quality of the input sequencing data is the single most important determinant of the quality of the final variant calls. Raw FASTQ files from sequencers contain various technical artifacts, including sequencing adapters, low-quality base calls, and PCR duplicates, which can introduce errors in the downstream alignment and variant calling stages.16 The initial quality assurance stage is therefore a non-negotiable prerequisite for any high-fidelity genomic analysis. Failure to properly clean the raw data will directly lead to an inflation of false-positive variant calls. For instance, un-trimmed adapter sequences can be misinterpreted as split-reads, creating spurious translocation or insertion calls, while un-flagged PCR duplicates can artificially inflate read depth, leading to false duplication calls by CNV callers.183.1 Initial Quality Control with FastQCThe first step is to assess the quality of the raw FASTQ files using FastQC. FastQC is a standard tool that provides a suite of metrics to quickly diagnose potential issues with sequencing data.19Bash# Create an output directory for the QC reports
mkdir -p qc/raw

# Run FastQC on the raw paired-end FASTQ files
fastqc -o qc/raw --threads 4 raw_reads/${SAMPLE}_R1.fastq.gz
fastqc -o qc/raw --threads 4 raw_reads/${SAMPLE}_R2.fastq.gz
This command generates an HTML report for each FASTQ file, which should be inspected for warnings or failures in key modules such as:Per Base Sequence Quality: Checks for drops in quality, particularly towards the 3' end of reads.Adapter Content: Detects the presence of adapter sequences that need to be removed.Per Base Sequence Content: Can indicate biases in nucleotide composition.3.2 Read Trimming and Filtering with TrimmomaticBased on the FastQC report, Trimmomatic is used to clean the reads. It is a flexible tool that can perform adapter removal, quality-based trimming, and length filtering in a single step.20Bash# Create an output directory for trimmed reads
mkdir -p trimmed_reads

# Run Trimmomatic
java -jar trimmomatic.jar PE -threads 4 \
    raw_reads/${SAMPLE}_R1.fastq.gz \
    raw_reads/${SAMPLE}_R2.fastq.gz \
    trimmed_reads/${SAMPLE}_R1.trimmed.fastq.gz \
    trimmed_reads/${SAMPLE}_R1.unpaired.fastq.gz \
    trimmed_reads/${SAMPLE}_R2.trimmed.fastq.gz \
    trimmed_reads/${SAMPLE}_R2.unpaired.fastq.gz \
    ILLUMINACLIP:adapters/TruSeq3-PE.fa:2:30:10 \
    SLIDINGWINDOW:4:20 \
    MINLEN:50
The key parameters used here are:PE: Specifies paired-end mode.ILLUMINACLIP:adapters/TruSeq3-PE.fa:2:30:10: Removes adapter sequences. The parameters specify the adapter FASTA file, the maximum number of seed mismatches (2), the palindrome clip threshold (30), and the simple clip threshold (10).SLIDINGWINDOW:4:20: Trims reads using a sliding window approach. It scans the read with a 4-base wide window and cuts when the average quality per base drops below 20.MINLEN:50: Discards reads that are shorter than 50 bases after trimming.3.3 Post-Trimming QC and Aggregation with MultiQCAfter trimming, it is crucial to run FastQC again to confirm that the quality issues have been resolved. The final step in this stage is to use MultiQC to aggregate all the FastQC reports (both raw and trimmed) into a single, interactive HTML report. This is particularly useful when processing multiple samples, as it provides a cohort-level overview of data quality.21Bash# Create an output directory for the trimmed QC reports
mkdir -p qc/trimmed

# Run FastQC on the trimmed reads
fastqc -o qc/trimmed --threads 4 trimmed_reads/${SAMPLE}_R1.trimmed.fastq.gz
fastqc -o qc/trimmed --threads 4 trimmed_reads/${SAMPLE}_R2.trimmed.fastq.gz

# Run MultiQC on the entire qc directory to aggregate all reports
multiqc qc -o qc/multiqc_report
By comparing the "before" and "after" reports within the MultiQC summary, one can verify the effectiveness of the pre-processing steps and proceed to the alignment stage with high-quality, analysis-ready reads.Section 4: Stage 2 - Genome Alignment and PreparationWith clean reads in hand, the next stage is to align them to a reference genome. This process generates the primary data structure for all downstream variant analysis: the BAM (Binary Alignment/Map) file. Proper processing and preparation of this file are critical for accuracy.4.1 Alignment to a Reference Genome with BWA-MEMThe Burrows-Wheeler Aligner's MEM algorithm (BWA-MEM) is the de facto industry standard for aligning short-read WGS data due to its speed, accuracy, and ability to generate split-read alignments necessary for SV calling.19Bash# Create an output directory for alignments
mkdir -p bam

# Run BWA-MEM and pipe directly to samtools for SAM-to-BAM conversion
# The -R option adds the read group header, which is required by many downstream tools.
# The -M option marks shorter split hits as secondary, for compatibility with Picard.
bwa mem -t 8 -M \
    -R "@RG\tID:${SAMPLE}\tSM:${SAMPLE}\tPL:ILLUMINA" \
    reference/hg38.fasta \
    trimmed_reads/${SAMPLE}_R1.trimmed.fastq.gz \
    trimmed_reads/${SAMPLE}_R2.trimmed.fastq.gz | \
    samtools view -Sb - > bam/${SAMPLE}.bam
This command aligns the paired-end reads to the hg38.fasta reference. The output is generated in SAM format and is immediately piped (|) to samtools view, which converts it to the more compact, compressed BAM format. This avoids writing a large intermediate SAM file to disk.4.2 Conversion, Sorting, and Indexing with SamtoolsThe BAM file produced by BWA is in the order the reads were processed. Most genomic tools require the file to be sorted by chromosomal coordinates for efficient random access. After sorting, an index file (.bai) must be created.Bash# Sort the BAM file by coordinate
samtools sort -@ 8 -o bam/${SAMPLE}.sorted.bam bam/${SAMPLE}.bam

# Index the sorted BAM file
samtools index -@ 8 bam/${SAMPLE}.sorted.bam
This creates bam/${SAMPLE}.sorted.bam and bam/${SAMPLE}.sorted.bam.bai. The original unsorted BAM file can now be removed.4.3 Marking Duplicate Reads with PicardDuring the library preparation process, PCR amplification can lead to multiple copies of the same original DNA fragment. These PCR duplicates can artificially inflate the read count in certain regions, which would be misinterpreted as a copy number gain by read-depth based CNV callers. The Picard MarkDuplicates tool identifies these duplicates based on their mapping coordinates and flags them in the BAM file.23 These flagged reads are then ignored by downstream tools.Bash# Create a directory for the final, analysis-ready BAM
mkdir -p analysis_ready_bam

# Run Picard MarkDuplicates
java -jar picard.jar MarkDuplicates \
    INPUT=bam/${SAMPLE}.sorted.bam \
    OUTPUT=analysis_ready_bam/${SAMPLE}.dedup.bam \
    METRICS_FILE=qc/${SAMPLE}.dedup_metrics.txt \
    CREATE_INDEX=true \
    VALIDATION_STRINGENCY=STRICT
This command produces the final, analysis-ready BAM file: analysis_ready_bam/${SAMPLE}.dedup.bam. This is the primary input file for all subsequent SV and CNV calling steps.4.4 (Optional but Recommended) Base Quality Score Recalibration (BQSR)While not strictly required for all SV callers, Base Quality Score Recalibration (BQSR) is a GATK best practice that can improve the accuracy of variant calls. The process involves building a model of systematic errors in the base quality scores produced by the sequencer and then adjusting the quality scores in the BAM file. Since some CNV callers may use SNV calls as part of their evidence (e.g., for calculating B-allele frequency), and because it is a standard step in GATK workflows, it is recommended for achieving the highest accuracy.25 The process involves two GATK steps: BaseRecalibrator followed by ApplyBQSR. Due to its complexity and reliance on known variant databases (like dbSNP), it is presented here as an optional enhancement rather than a core component of the bash script.Section 5: Stage 3 - Ensemble Structural Variant (SV) CallingThis stage employs three distinct SV callers that leverage paired-end and split-read evidence. Running multiple callers and integrating their results provides a more comprehensive and robust callset than any single tool can achieve. This ensemble approach helps to mitigate the algorithmic biases inherent in any single caller and increases confidence in variants detected by more than one method.115.1 Manta: High-Precision Breakpoint DetectionManta is a fast and accurate SV caller developed by Illumina. It is designed to discover, genotype, and visualize the full spectrum of SVs from mapped paired-end sequencing reads. It identifies candidate SVs from discordant read-pair and split-read alignments and then performs local assembly and realignment to refine breakpoints, resulting in high-precision calls.9 It is often considered a top-performing open-source algorithm for short-read data.26Bash# Create an output directory for Manta
mkdir -p sv_calls/manta

# Step 1: Configure the Manta analysis workflow
configManta.py \
    --bam analysis_ready_bam/${SAMPLE}.dedup.bam \
    --referenceFasta reference/hg38.fasta \
    --runDir sv_calls/manta/${SAMPLE}

# Step 2: Execute the Manta workflow
python sv_calls/manta/${SAMPLE}/runWorkflow.py -m local -j 8
Manta's output includes several VCF files. The key result for downstream analysis is diploidSV.vcf.gz, which contains the final, filtered set of SVs and indels for the sample.5.2 Delly: An Integrated PE/SR CallerDelly is another widely used SV discovery method that integrates paired-ends and split-reads to accurately delineate genomic rearrangements at single-nucleotide resolution.10 It is capable of detecting deletions, tandem duplications, inversions, and translocations.Bash# Create an output directory for Delly
mkdir -p sv_calls/delly

# Run Delly for each SV type
# The -g flag specifies the reference genome FASTA
# The -o flag specifies the output file
# The final argument is the input BAM file
delly call -t DEL -g reference/hg38.fasta -o sv_calls/delly/${SAMPLE}.del.bcf analysis_ready_bam/${SAMPLE}.dedup.bam
delly call -t DUP -g reference/hg38.fasta -o sv_calls/delly/${SAMPLE}.dup.bcf analysis_ready_bam/${SAMPLE}.dedup.bam
delly call -t INV -g reference/hg38.fasta -o sv_calls/delly/${SAMPLE}.inv.bcf analysis_ready_bam/${SAMPLE}.dedup.bam
delly call -t TRA -g reference/hg38.fasta -o sv_calls/delly/${SAMPLE}.tra.bcf analysis_ready_bam/${SAMPLE}.dedup.bam

# Merge the BCF files for each SV type into a single file
bcftools concat -a \
    sv_calls/delly/${SAMPLE}.del.bcf \
    sv_calls/delly/${SAMPLE}.dup.bcf \
    sv_calls/delly/${SAMPLE}.inv.bcf \
    sv_calls/delly/${SAMPLE}.tra.bcf | \
    bcftools sort -Oz -o sv_calls/delly/${SAMPLE}.merged.vcf.gz

# Index the final VCF
tabix -p vcf sv_calls/delly/${SAMPLE}.merged.vcf.gz
Delly is run separately for each SV type, and the resulting BCF (binary VCF) files are then merged and sorted using BCFtools to create a final VCF file for the sample.5.3 Lumpy: A Probabilistic FrameworkLumpy (also known as Lumpy-SV) is a probabilistic framework that integrates multiple evidence types, including discordant paired-ends, split-reads, and read-depth information, to call SVs.11 A key pre-processing step for Lumpy is the extraction of the specific reads that provide SV evidence from the main BAM file.30Bash# Create an output directory for Lumpy
mkdir -p sv_calls/lumpy

# Step 1: Extract discordant read pairs
samtools view -b -F 1294 analysis_ready_bam/${SAMPLE}.dedup.bam > sv_calls/lumpy/${SAMPLE}.discordants.bam

# Step 2: Extract split-reads
samtools view -h analysis_ready_bam/${SAMPLE}.dedup.bam | \
    extractSplitReads_BwaMem -i stdin | \
    samtools view -Sb - > sv_calls/lumpy/${SAMPLE}.splitters.bam

# Step 3: Sort the discordant and splitter BAMs
samtools sort -@ 8 -o sv_calls/lumpy/${SAMPLE}.discordants.sorted.bam sv_calls/lumpy/${SAMPLE}.discordants.bam
samtools sort -@ 8 -o sv_calls/lumpy/${SAMPLE}.splitters.sorted.bam sv_calls/lumpy/${SAMPLE}.splitters.bam

# Step 4: Run Lumpy Express (a wrapper script for convenience)
lumpyexpress \
    -B analysis_ready_bam/${SAMPLE}.dedup.bam \
    -S sv_calls/lumpy/${SAMPLE}.splitters.sorted.bam \
    -D sv_calls/lumpy/${SAMPLE}.discordants.sorted.bam \
    -o sv_calls/lumpy/${SAMPLE}.vcf
The lumpyexpress script simplifies the process of running Lumpy by automatically calculating library insert size statistics and passing the necessary parameters to the main lumpy executable. The output is a single VCF file containing all SV calls.5.4 Comparison of SV Calling AlgorithmsThe selection of Manta, Delly, and Lumpy for the ensemble SV calling stage is strategic, designed to leverage their diverse algorithmic approaches to maximize detection power.ToolPrimary Signal(s)Key StrengthPrimary OutputMantaPaired-End (PE) + Split-Read (SR)Fast, high-precision breakpoints via local assembly, discovers all SV types including large indels.9Filtered diploid SV VCF (diploidSV.vcf.gz).DellyPaired-End (PE) + Split-Read (SR)Robust genotyping, designed for single-sample and cohort analysis, detects all SV types.10Per-type BCF files, merged into a final VCF.LumpyPE + SR + Read-Depth (RD)Probabilistic framework integrates multiple signals, flexible for joint-calling multiple samples.11Single VCF with evidence details in INFO/FORMAT fields.By combining these three callers, the pipeline captures a more complete and reliable set of structural variants than would be possible with any single tool.Section 6: Stage 4 - Read-Depth Copy Number Variation (CNV) CallingWhile the SV callers in the previous stage can detect deletions and duplications, they are primarily optimized for breakpoint discovery. For robust quantification of copy number, dedicated read-depth (RD) based methods are superior. This stage employs two complementary RD callers.6.1 CNVnator: A Read-Depth Binning ApproachCNVnator is a widely used tool for CNV discovery based on read-depth analysis.32 It works by dividing the genome into non-overlapping bins of a user-defined size, counting the mapped reads within each bin, correcting for GC-content biases, and then using a mean-shift statistical approach to partition the genome into segments of differing copy number.12 The choice of bin size is critical and depends on sequencing coverage; a smaller bin size provides higher resolution but requires higher coverage.13 For typical 30x WGS, a bin size of 1000 bp is a reasonable starting point.Bash# Create an output directory for CNVnator
mkdir -p cnv_calls/cnvnator

# Path to the CNVnator executable
CNVNATOR_PATH="/path/to/cnvnator"
BIN_SIZE=1000

# Step 1: Extract read mapping signal from BAM into a ROOT file
${CNVNATOR_PATH}/cnvnator -root ${SAMPLE}.root -tree analysis_ready_bam/${SAMPLE}.dedup.bam -chrom $(seq 1 22) X Y

# Step 2: Generate a read-depth histogram
${CNVNATOR_PATH}/cnvnator -root ${SAMPLE}.root -his ${BIN_SIZE} -d reference/hg38_chrom_dirs

# Step 3: Calculate statistics
${CNVNATOR_PATH}/cnvnator -root ${SAMPLE}.root -stat ${BIN_SIZE}

# Step 4: Partition the signal
${CNVNATOR_PATH}/cnvnator -root ${SAMPLE}.root -partition ${BIN_SIZE}

# Step 5: Call CNVs
${CNVNATOR_PATH}/cnvnator -root ${SAMPLE}.root -call ${BIN_SIZE} > cnv_calls/cnvnator/${SAMPLE}.cnvnator.out

# Step 6 (Optional but recommended): Convert CNVnator output to VCF
cnvnator2VCF.pl cnv_calls/cnvnator/${SAMPLE}.cnvnator.out > cnv_calls/cnvnator/${SAMPLE}.vcf
This multi-step process generates a final output file that lists the predicted CNVs with their coordinates, size, and statistical significance. The provided cnvnator2VCF.pl script can convert this native format into a standard VCF file for easier integration with other tools.6.2 GATK gCNV: A Probabilistic Cohort-Based ModelThe GATK GermlineCNVCaller (gCNV) offers a more advanced approach to CNV detection. It uses a Bayesian probabilistic model to account for various sources of noise and bias in WGS data, such as GC content and mappability.13 While it can be run on a single sample (case mode), its true power is realized in cohort mode, where it analyzes many samples together to build a "panel of normals." This model of normal coverage variation allows it to detect rare CNVs with much higher sensitivity and specificity than methods that rely on a single sample's genome-wide average.13 The bash script here will demonstrate the essential steps, which are conceptually applicable to both modes.Bash# Create an output directory for GATK gCNV
mkdir -p cnv_calls/gatk_gcnv

# For WGS, a bin length of 1000 is recommended for 30x coverage
BIN_LENGTH=1000

# Step 1: Preprocess intervals to create bins across the genome
gatk PreprocessIntervals \
    -R reference/hg38.fasta \
    --bin-length ${BIN_LENGTH} \
    --padding 0 \
    -O cnv_calls/gatk_gcnv/hg38.preprocessed.interval_list

# Step 2: Collect read counts in the bins for our sample
gatk CollectReadCounts \
    -I analysis_ready_bam/${SAMPLE}.dedup.bam \
    -L cnv_calls/gatk_gcnv/hg38.preprocessed.interval_list \
    --interval-merging-rule OVERLAPPING_ONLY \
    -O cnv_calls/gatk_gcnv/${SAMPLE}.counts.hdf5

# Step 3: Determine baseline contig ploidy for the sample
# In cohort mode, a ploidy model from the cohort would be used here.
gatk DetermineGermlineContigPloidy \
    -I cnv_calls/gatk_gcnv/${SAMPLE}.counts.hdf5 \
    --contig-ploidy-priors reference/contig_ploidy_priors.tsv \
    --output cnv_calls/gatk_gcnv/${SAMPLE}-ploidy-calls \
    --output-prefix ${SAMPLE}

# Step 4: Call CNVs
# This step is shown in "case mode". In cohort mode, a model trained on many samples is provided.
gatk GermlineCNVCaller \
    --run-mode CASE \
    -I cnv_calls/gatk_gcnv/${SAMPLE}.counts.hdf5 \
    --contig-ploidy-calls cnv_calls/gatk_gcnv/${SAMPLE}-ploidy-calls \
    --output cnv_calls/gatk_gcnv/${SAMPLE}-gcnv-calls \
    --output-prefix ${SAMPLE}

# Step 5: Post-process calls to generate final VCF
gatk PostprocessGermlineCNVCalls \
    --model-shard-path cnv_calls/gatk_gcnv/${SAMPLE}-gcnv-calls/shard-0 \
    --calls-shard-path cnv_calls/gatk_gcnv/${SAMPLE}-gcnv-calls/shard-0 \
    --allosomal-contig X --allosomal-contig Y \
    --contig-ploidy-calls cnv_calls/gatk_gcnv/${SAMPLE}-ploidy-calls \
    --sample-index 0 \
    --output-genotyped-intervals cnv_calls/gatk_gcnv/${SAMPLE}.intervals.vcf \
    --output-genotyped-segments cnv_calls/gatk_gcnv/${SAMPLE}.segments.vcf
This workflow produces a VCF file (.segments.vcf) containing the final CNV calls for the sample.6.3 Comparison of CNV Calling AlgorithmsThe choice between CNVnator and GATK-gCNV depends on the experimental design and analytical goals. Their distinct methodologies offer different advantages.ToolMethodologyStrengthsIdeal Use CaseCNVnatorMean-shift on binned read-depth (RD)Fast, straightforward, effective for single-sample analysis, good for detecting large CNVs.12Quick analysis of single samples; somatic CNV detection in tumor-only data.GATK-gCNVBayesian Hidden Markov Model (HMM) on cohort RDModels technical noise and coverage biases, highly sensitive for rare germline events, robustly handles sex chromosomes.13Analysis of germline CNVs in cohorts (e.g., family trios, population studies).For a comprehensive analysis, running both can be beneficial. CNVnator provides a rapid, robust overview, while GATK-gCNV (especially in cohort mode) can provide higher sensitivity for subtle or rare germline events.Section 7: Stage 5 - Callset Integration and RefinementHaving generated raw calls from five different tools, the next critical stage is to integrate these results and apply stringent filters to produce a high-confidence, analysis-ready callset. This stage is where the trade-off between sensitivity (discovering all true variants) and specificity (avoiding false positives) is managed.7.1 Merging SV Callsets with SURVIVORThe VCF files from Manta, Delly, and Lumpy must be merged into a single, non-redundant SV callset. SURVIVOR is a purpose-built tool for comparing, merging, and filtering SV callsets from different callers.35Bash# Create an output directory for merged results
mkdir -p merged_filtered_sv

# Create a file listing the VCFs to be merged
ls sv_calls/manta/${SAMPLE}/results/variants/diploidSV.vcf.gz \
   sv_calls/delly/${SAMPLE}.merged.vcf.gz \
   sv_calls/lumpy/${SAMPLE}.vcf > sv_vcf_list.txt

# Run SURVIVOR merge
# Parameters: vcf_list.txt max_dist min_callers type_match strand_match size_match min_size output.vcf
SURVIVOR merge sv_vcf_list.txt 1000 2 1 1 0 50 merged_filtered_sv/${SAMPLE}.sv.merged.vcf
The parameters for SURVIVOR merge are key to defining the stringency of the merge 36:1000: Maximum distance in base pairs between breakpoints for two SVs to be considered the same event.2: The minimum number of callers that must support an SV for it to be included in the merged output. This is a powerful filter for increasing confidence. Setting it to 1 would create a union of all calls (high sensitivity, more noise), while 3 would create a high-confidence intersection (high specificity, lower sensitivity).1 1 0: These flags specify to take SV type, strand, and size into account during merging.50: The minimum size of an SV to be considered (50 bp).7.2 Filtering VCFs with BCFtoolsBCFtools is a powerful suite for manipulating VCF files. Its filtering capabilities allow for the application of complex, expression-based rules to remove low-quality or likely artifactual calls.38 Filtering should be applied to the merged SV callset and the individual CNV callsets.Bash# --- Filtering the Merged SV Callset ---

# Example: Filter for SVs that are not BNDs (translocations), have a QUAL score > 30,
# and are supported by more than 5 paired-end reads.
bcftools filter -i 'SVTYPE!="BND" && QUAL>30 && INFO/PE>5' \
    merged_filtered_sv/${SAMPLE}.sv.merged.vcf | \
    bgzip -c > merged_filtered_sv/${SAMPLE}.sv.merged.filtered.vcf.gz

tabix -p vcf merged_filtered_sv/${SAMPLE}.sv.merged.filtered.vcf.gz


# --- Filtering a CNV Callset (e.g., from CNVnator) ---

# Example: Filter for CNVs with a q0 value < 0.5 (statistical test) and a size > 5000 bp.
# The exact INFO fields will depend on the VCF conversion script used for CNVnator.
bcftools filter -i 'INFO/Q0<0.5 && INFO/SVLEN>5000' \
    cnv_calls/cnvnator/${SAMPLE}.vcf | \
    bgzip -c > merged_filtered_sv/${SAMPLE}.cnv.filtered.vcf.gz

tabix -p vcf merged_filtered_sv/${SAMPLE}.cnv.filtered.vcf.gz
Additional, more advanced filtering strategies are highly recommended:Blacklist Filtering: Exclude calls that overlap with problematic genomic regions, such as centromeres, telomeres, and segmental duplications, where short-read alignment is unreliable. This can be done using bcftools filter -T blacklist.bed.Population Frequency Filtering: To enrich for rare, potentially disease-causing variants, one can filter out common SVs present in population databases like gnomAD-SV. This involves annotating the VCF with frequencies from the database and then filtering based on a threshold (e.g., INFO/AF < 0.01).The specific filtering parameters must be tuned based on the dataset, sequencing depth, and the goals of the study. The commands above provide a template for constructing a robust filtering strategy.Section 8: Stage 6 - Functional Annotation and PrioritizationThe final stage of the pipeline is to annotate the filtered, high-confidence variant calls with functional information to facilitate biological interpretation. This involves determining which genes and regulatory elements are impacted by the SVs/CNVs and predicting their potential pathogenicity.8.1 Comprehensive SV Annotation with AnnotSVAnnotSV is a powerful tool specifically designed for the functional annotation and ranking of structural variations.40 It integrates a vast array of data sources, including gene definitions (RefSeq, Ensembl), disease databases (OMIM, ClinVar), population frequency databases (DGV, 1000 Genomes), and regulatory element tracks (ENCODE, GeneHancer). Crucially, it provides a classification of SVs based on the American College of Medical Genetics and Genomics (ACMG) guidelines, categorizing them from "Benign" to "Pathogenic".42Bash# Create an output directory for annotations
mkdir -p annotations

# Run AnnotSV on the final filtered SV callset
# ANNOTSV_ROOT should be the path to the AnnotSV installation
$ANNOTSV_ROOT/bin/AnnotSV \
    -SVinputFile merged_filtered_sv/${SAMPLE}.sv.merged.filtered.vcf.gz \
    -outputFile annotations/${SAMPLE}.sv.annotsv.tsv \
    -genomeBuild GRCh38
AnnotSV produces a detailed tab-separated (TSV) output file. It offers two annotation modes: "full" mode, which provides one line per SV event, and "split" mode, which provides a separate line for each gene overlapped by a single SV.42 This dual output is extremely useful for both event-centric and gene-centric interpretation.8.2 Consequence Prediction with Ensembl VEPThe Ensembl Variant Effect Predictor (VEP) is a complementary tool that excels at predicting the specific molecular consequences of variants on transcripts.43 For SVs, it can predict high-impact consequences such as transcript_ablation (if a gene is deleted) or feature_truncation. It also seamlessly integrates allele frequency data from gnomAD and pathogenicity predictions from tools like SIFT and PolyPhen (for SNVs).43Bash# Run VEP on the final filtered SV callset
# Assumes VEP and the necessary cache for GRCh38 have been installed
vep --input_file merged_filtered_sv/${SAMPLE}.sv.merged.filtered.vcf.gz \
    --output_file annotations/${SAMPLE}.sv.vep.vcf \
    --vcf \
    --cache \
    --offline \
    --fork 8 \
    --fasta reference/hg38.fasta \
    --symbol \
    --terms SO \
    --hgvs
VEP adds its annotations to the INFO field of the VCF file under the CSQ (Consequence) key. Using both AnnotSV and VEP provides a comprehensive, multi-layered annotation that is essential for prioritizing candidate variants for further investigation.Section 9: The Complete sv_cnv_pipeline.sh ScriptThe following script consolidates all the stages described above into a single, executable bash script. It is structured with variables for key paths and parameters, includes basic error checking, and uses functions to modularize the workflow.Bash#!/bin/bash

# A comprehensive pipeline for SV and CNV analysis from WGS data.
# This script is for demonstration and single-sample analysis.
# For production-level, multi-sample analysis, a workflow manager like Snakemake is recommended.

# --- Configuration ---
set -e # Exit immediately if a command exits with a non-zero status.
set -o pipefail # The return value of a pipeline is the status of the last command to exit with a non-zero status.

# User-defined variables
SAMPLE="NA12878" # Example sample ID
THREADS=8

# Paths to tools and reference files
TRIMMOMATIC_JAR="/path/to/trimmomatic.jar"
PICARD_JAR="/path/to/picard.jar"
CNVNATOR_PATH="/path/to/cnvnator/src"
ANNOTSV_ROOT="/path/to/AnnotSV"
REF_GENOME="reference/hg38.fasta"
ADAPTERS="adapters/TruSeq3-PE.fa"

# --- Pipeline Functions ---

# Stage 1: QC and Trimming
run_qc_and_trim() {
    echo "--- STAGE 1: QC and Trimming ---"
    # Initial QC
    mkdir -p qc/raw
    fastqc -o qc/raw --threads ${THREADS} raw_reads/${SAMPLE}_R1.fastq.gz
    fastqc -o qc/raw --threads ${THREADS} raw_reads/${SAMPLE}_R2.fastq.gz

    # Trimming
    mkdir -p trimmed_reads
    java -jar ${TRIMMOMATIC_JAR} PE -threads ${THREADS} \
        raw_reads/${SAMPLE}_R1.fastq.gz raw_reads/${SAMPLE}_R2.fastq.gz \
        trimmed_reads/${SAMPLE}_R1.trimmed.fastq.gz trimmed_reads/${SAMPLE}_R1.unpaired.fastq.gz \
        trimmed_reads/${SAMPLE}_R2.trimmed.fastq.gz trimmed_reads/${SAMPLE}_R2.unpaired.fastq.gz \
        ILLUMINACLIP:${ADAPTERS}:2:30:10 SLIDINGWINDOW:4:20 MINLEN:50

    # Post-trimming QC
    mkdir -p qc/trimmed
    fastqc -o qc/trimmed --threads ${THREADS} trimmed_reads/${SAMPLE}_R1.trimmed.fastq.gz
    fastqc -o qc/trimmed --threads ${THREADS} trimmed_reads/${SAMPLE}_R2.trimmed.fastq.gz

    # Aggregate QC reports
    multiqc qc -o qc/multiqc_report
    echo "--- STAGE 1 Complete ---"
}

# Stage 2: Alignment and BAM Preparation
run_alignment() {
    echo "--- STAGE 2: Alignment and BAM Preparation ---"
    mkdir -p bam analysis_ready_bam

    # BWA-MEM alignment
    bwa mem -t ${THREADS} -M -R "@RG\tID:${SAMPLE}\tSM:${SAMPLE}\tPL:ILLUMINA" \
        ${REF_GENOME} \
        trimmed_reads/${SAMPLE}_R1.trimmed.fastq.gz \
        trimmed_reads/${SAMPLE}_R2.trimmed.fastq.gz | \
        samtools view -Sb - > bam/${SAMPLE}.bam

    # Sort and index
    samtools sort -@ ${THREADS} -o bam/${SAMPLE}.sorted.bam bam/${SAMPLE}.bam
    samtools index -@ ${THREADS} bam/${SAMPLE}.sorted.bam

    # Mark duplicates
    java -jar ${PICARD_JAR} MarkDuplicates \
        INPUT=bam/${SAMPLE}.sorted.bam \
        OUTPUT=analysis_ready_bam/${SAMPLE}.dedup.bam \
        METRICS_FILE=qc/${SAMPLE}.dedup_metrics.txt \
        CREATE_INDEX=true \
        VALIDATION_STRINGENCY=STRICT
    echo "--- STAGE 2 Complete ---"
}

# Stage 3: SV Calling
run_sv_calling() {
    echo "--- STAGE 3: Ensemble SV Calling ---"
    # Manta
    echo "Running Manta..."
    mkdir -p sv_calls/manta
    configManta.py --bam analysis_ready_bam/${SAMPLE}.dedup.bam --referenceFasta ${REF_GENOME} --runDir sv_calls/manta/${SAMPLE}
    python sv_calls/manta/${SAMPLE}/runWorkflow.py -m local -j ${THREADS}

    # Delly
    echo "Running Delly..."
    mkdir -p sv_calls/delly
    delly call -t DEL -g ${REF_GENOME} -o sv_calls/delly/${SAMPLE}.del.bcf analysis_ready_bam/${SAMPLE}.dedup.bam
    delly call -t DUP -g ${REF_GENOME} -o sv_calls/delly/${SAMPLE}.dup.bcf analysis_ready_bam/${SAMPLE}.dedup.bam
    delly call -t INV -g ${REF_GENOME} -o sv_calls/delly/${SAMPLE}.inv.bcf analysis_ready_bam/${SAMPLE}.dedup.bam
    delly call -t TRA -g ${REF_GENOME} -o sv_calls/delly/${SAMPLE}.tra.bcf analysis_ready_bam/${SAMPLE}.dedup.bam
    bcftools concat -a sv_calls/delly/*.bcf | bcftools sort -Oz -o sv_calls/delly/${SAMPLE}.merged.vcf.gz
    tabix -p vcf sv_calls/delly/${SAMPLE}.merged.vcf.gz

    # Lumpy
    echo "Running Lumpy..."
    mkdir -p sv_calls/lumpy
    samtools view -b -F 1294 analysis_ready_bam/${SAMPLE}.dedup.bam > sv_calls/lumpy/${SAMPLE}.discordants.bam
    samtools view -h analysis_ready_bam/${SAMPLE}.dedup.bam | extractSplitReads_BwaMem -i stdin | samtools view -Sb - > sv_calls/lumpy/${SAMPLE}.splitters.bam
    samtools sort -@ ${THREADS} -o sv_calls/lumpy/${SAMPLE}.discordants.sorted.bam sv_calls/lumpy/${SAMPLE}.discordants.bam
    samtools sort -@ ${THREADS} -o sv_calls/lumpy/${SAMPLE}.splitters.sorted.bam sv_calls/lumpy/${SAMPLE}.splitters.bam
    lumpyexpress -B analysis_ready_bam/${SAMPLE}.dedup.bam -S sv_calls/lumpy/${SAMPLE}.splitters.sorted.bam -D sv_calls/lumpy/${SAMPLE}.discordants.sorted.bam -o sv_calls/lumpy/${SAMPLE}.vcf

    echo "--- STAGE 3 Complete ---"
}

# Stage 4: CNV Calling
run_cnv_calling() {
    echo "--- STAGE 4: Read-Depth CNV Calling ---"
    # CNVnator
    echo "Running CNVnator..."
    mkdir -p cnv_calls/cnvnator
    BIN_SIZE=1000
    ${CNVNATOR_PATH}/cnvnator -root cnv_calls/cnvnator/${SAMPLE}.root -tree analysis_ready_bam/${SAMPLE}.dedup.bam -chrom $(seq 1 22) X Y
    ${CNVNATOR_PATH}/cnvnator -root cnv_calls/cnvnator/${SAMPLE}.root -his ${BIN_SIZE} -d reference/hg38_chrom_dirs
    ${CNVNATOR_PATH}/cnvnator -root cnv_calls/cnvnator/${SAMPLE}.root -stat ${BIN_SIZE}
    ${CNVNATOR_PATH}/cnvnator -root cnv_calls/cnvnator/${SAMPLE}.root -partition ${BIN_SIZE}
    ${CNVNATOR_PATH}/cnvnator -root cnv_calls/cnvnator/${SAMPLE}.root -call ${BIN_SIZE} > cnv_calls/cnvnator/${SAMPLE}.cnvnator.out
    cnvnator2VCF.pl cnv_calls/cnvnator/${SAMPLE}.cnvnator.out > cnv_calls/cnvnator/${SAMPLE}.vcf
    echo "--- STAGE 4 Complete ---"
}

# Stage 5: Integration and Filtering
run_integration_and_filtering() {
    echo "--- STAGE 5: Integration and Filtering ---"
    mkdir -p merged_filtered_sv

    # Merge SV calls with SURVIVOR
    ls sv_calls/manta/${SAMPLE}/results/variants/diploidSV.vcf.gz \
       sv_calls/delly/${SAMPLE}.merged.vcf.gz \
       sv_calls/lumpy/${SAMPLE}.vcf > sv_vcf_list.txt
    SURVIVOR merge sv_vcf_list.txt 1000 2 1 1 0 50 merged_filtered_sv/${SAMPLE}.sv.merged.vcf

    # Filter merged SV calls
    bcftools filter -i 'SVTYPE!="BND" && QUAL>30 && INFO/PE>5' merged_filtered_sv/${SAMPLE}.sv.merged.vcf | \
        bgzip -c > merged_filtered_sv/${SAMPLE}.sv.merged.filtered.vcf.gz
    tabix -p vcf merged_filtered_sv/${SAMPLE}.sv.merged.filtered.vcf.gz

    # Filter CNV calls
    bcftools filter -i 'INFO/Q0<0.5 && INFO/SVLEN>5000' cnv_calls/cnvnator/${SAMPLE}.vcf | \
        bgzip -c > merged_filtered_sv/${SAMPLE}.cnv.filtered.vcf.gz
    tabix -p vcf merged_filtered_sv/${SAMPLE}.cnv.filtered.vcf.gz
    echo "--- STAGE 5 Complete ---"
}

# Stage 6: Annotation
run_annotation() {
    echo "--- STAGE 6: Functional Annotation ---"
    mkdir -p annotations

    # AnnotSV
    echo "Running AnnotSV..."
    ${ANNOTSV_ROOT}/bin/AnnotSV \
        -SVinputFile merged_filtered_sv/${SAMPLE}.sv.merged.filtered.vcf.gz \
        -outputFile annotations/${SAMPLE}.sv.annotsv.tsv \
        -genomeBuild GRCh38

    # VEP
    echo "Running VEP..."
    vep --input_file merged_filtered_sv/${SAMPLE}.sv.merged.filtered.vcf.gz \
        --output_file annotations/${SAMPLE}.sv.vep.vcf \
        --vcf --cache --offline --fork ${THREADS} --fasta ${REF_GENOME} \
        --symbol --terms SO --hgvs
    echo "--- STAGE 6 Complete ---"
}

# --- Main Execution ---
run_qc_and_trim
run_alignment
run_sv_calling
run_cnv_calling
run_integration_and_filtering
run_annotation

echo "--- PIPELINE FINISHED SUCCESSFULLY ---"
Part III: The Snakemake Pipeline: A Framework for Reproducible and Scalable AnalysisWhile the bash script provides a clear, linear path through the analysis, it lacks scalability, error recovery, and true reproducibility. For any project involving more than a handful of samples, or for research where computational reproducibility is paramount, a dedicated workflow management system is essential. This part describes the implementation of the same comprehensive SV/CNV pipeline using Snakemake, a state-of-the-art workflow manager.Section 10: Principles of the Snakemake Framework10.1 From Imperative Scripts to Declarative WorkflowsA bash script is imperative: it is a list of commands that must be executed in a specific order, and the user is responsible for ensuring that order. Snakemake, in contrast, is declarative. The user defines a set of rules, where each rule describes how to create a specific output file from a set of input files.45 Snakemake then analyzes these rules to build a Directed Acyclic Graph (DAG) of all the jobs that need to be run.15 It automatically determines the correct execution order and can identify which jobs can be run in parallel. If a run is interrupted, or if an input file changes, Snakemake is intelligent enough to re-run only the necessary downstream steps, saving significant time and computational resources.10.2 Core Concepts: Rules, Wildcards, and ConfigurationThe Snakemake workflow is primarily defined in a file named Snakefile. Its key components are:Rules: A rule is the fundamental unit of a workflow. It has a name and specifies input files, output files, and a command to be executed in a shell or run block.14Pythonrule samtools_sort:
    input: "bam/{sample}.bam"
    output: "bam/{sample}.sorted.bam"
    shell: "samtools sort -o {output} {input}"
Wildcards: Wildcards are the mechanism that makes Snakemake pipelines generic and scalable. They are placeholders in file paths, enclosed in curly braces (e.g., {sample}). When Snakemake needs to create a specific file, like bam/NA12878.sorted.bam, it matches this pattern to the output of the samtools_sort rule and infers that the wildcard sample must be NA12878. It then uses this value to determine the required input file (bam/NA12878.bam) and executes the rule.46Configuration: To maintain a clean separation between the workflow logic and project-specific details, it is a best practice to use a configuration file (e.g., config.yaml).47 This file stores parameters like file paths, tool settings, and sample lists, which are then loaded into the Snakefile. This allows the same Snakefile to be used for different projects simply by changing the configuration file.10.3 Ensuring Reproducibility with Integrated Environment ManagementOne of the greatest challenges in bioinformatics is ensuring that an analysis can be reproduced later or by others. This is often difficult due to complex software dependencies and version conflicts. Snakemake provides an elegant solution to this problem through its direct integration with the Conda package manager.14Each rule can have a conda directive that points to a YAML file listing the specific software and versions required for that rule.Pythonrule bwa_map:
    input:...
    output:...
    conda: "envs/alignment.yaml"
    shell: "bwa mem..."
When Snakemake executes this rule, it will automatically create a new, isolated Conda environment from the alignment.yaml file, activate it, run the command, and then deactivate it. This guarantees that every step of the pipeline is always run with the exact same software versions, making the entire analysis fully portable and computationally reproducible on any system with Snakemake and Conda installed.49 This feature elevates the pipeline from a simple script to a robust, scientific-grade workflow.Section 11: The Snakefile: A Modular, Rule-Based ImplementationThe following Snakefile implements the full SV/CNV pipeline. It is structured to be readable and modular, with each rule corresponding to a specific task. The workflow is driven by a config.yaml file and a samples.tsv file, which define the parameters and input samples, respectively.Python# Snakefile for Comprehensive SV and CNV Analysis

import pandas as pd

# --- Configuration ---
configfile: "config.yaml"

# Load sample information from a TSV file
SAMPLES_TSV = pd.read_csv(config["samples_tsv"], sep="\t").set_index("sample", drop=False)
SAMPLES = list(SAMPLES_TSV.index)

# --- Target Rule ---
# This rule defines the final desired output files of the entire pipeline.
rule all:
    input:
        expand("qc/multiqc_report/multiqc_report.html"),
        expand("annotations/{sample}.sv.annotsv.tsv", sample=SAMPLES),
        expand("annotations/{sample}.sv.vep.vcf", sample=SAMPLES),
        expand("merged_filtered_sv/{sample}.cnv.filtered.vcf.gz", sample=SAMPLES)

# --- Stage 1: QC and Trimming ---
rule fastqc_raw:
    input:
        r1="raw_reads/{sample}_R1.fastq.gz",
        r2="raw_reads/{sample}_R2.fastq.gz"
    output:
        r1_html="qc/raw/{sample}_R1_fastqc.html",
        r2_html="qc/raw/{sample}_R2_fastqc.html"
    params:
        outdir="qc/raw"
    conda: "envs/qc.yaml"
    threads: 4
    shell: "fastqc -o {params.outdir} --threads {threads} {input.r1} {input.r2}"

rule trimmomatic:
    input:
        r1="raw_reads/{sample}_R1.fastq.gz",
        r2="raw_reads/{sample}_R2.fastq.gz"
    output:
        r1_trimmed="trimmed_reads/{sample}_R1.trimmed.fastq.gz",
        r2_trimmed="trimmed_reads/{sample}_R2.trimmed.fastq.gz",
        r1_unpaired="trimmed_reads/{sample}_R1.unpaired.fastq.gz",
        r2_unpaired="trimmed_reads/{sample}_R2.unpaired.fastq.gz"
    params:
        adapters=config["adapters_fasta"]
    conda: "envs/trimming.yaml"
    threads: 8
    shell:
        "trimmomatic PE -threads {threads} {input.r1} {input.r2} "
        "{output.r1_trimmed} {output.r1_unpaired} {output.r2_trimmed} {output.r2_unpaired} "
        "ILLUMINACLIP:{params.adapters}:2:30:10 SLIDINGWINDOW:4:20 MINLEN:50"

rule fastqc_trimmed:
    input:
        r1="trimmed_reads/{sample}_R1.trimmed.fastq.gz",
        r2="trimmed_reads/{sample}_R2.trimmed.fastq.gz"
    output:
        r1_html="qc/trimmed/{sample}_R1.trimmed_fastqc.html",
        r2_html="qc/trimmed/{sample}_R2.trimmed_fastqc.html"
    params:
        outdir="qc/trimmed"
    conda: "envs/qc.yaml"
    threads: 4
    shell: "fastqc -o {params.outdir} --threads {threads} {input.r1} {input.r2}"

rule multiqc:
    input:
        expand("qc/raw/{sample}_{read}_fastqc.html", sample=SAMPLES, read=),
        expand("qc/trimmed/{sample}_{read}.trimmed_fastqc.html", sample=SAMPLES, read=)
    output:
        "qc/multiqc_report/multiqc_report.html"
    params:
        qc_dir="qc"
    conda: "envs/qc.yaml"
    shell: "multiqc {params.qc_dir} -o qc/multiqc_report"

# --- Stage 2: Alignment and BAM Preparation ---
rule bwa_map:
    input:
        r1="trimmed_reads/{sample}_R1.trimmed.fastq.gz",
        r2="trimmed_reads/{sample}_R2.trimmed.fastq.gz",
        ref=config["reference_fasta"]
    output:
        bam=temp("bam/{sample}.bam")
    params:
        rg=r"@RG\tID:{sample}\tSM:{sample}\tPL:ILLUMINA"
    conda: "envs/alignment.yaml"
    threads: 16
    shell:
        "bwa mem -t {threads} -M -R '{params.rg}' {input.ref} {input.r1} {input.r2} | "
        "samtools view -Sb - > {output.bam}"

rule samtools_sort:
    input: "bam/{sample}.bam"
    output: temp("bam/{sample}.sorted.bam")
    conda: "envs/alignment.yaml"
    threads: 8
    shell: "samtools sort -@ {threads} -o {output} {input}"

rule picard_markdups:
    input: "bam/{sample}.sorted.bam"
    output:
        bam="analysis_ready_bam/{sample}.dedup.bam",
        bai="analysis_ready_bam/{sample}.dedup.bam.bai"
    params:
        metrics="qc/{sample}.dedup_metrics.txt"
    conda: "envs/alignment.yaml"
    shell:
        "picard MarkDuplicates "
        "INPUT={input} OUTPUT={output.bam} METRICS_FILE={params.metrics} "
        "CREATE_INDEX=true VALIDATION_STRINGENCY=STRICT"

# --- Stage 3: Ensemble SV Calling ---
rule manta:
    input:
        bam="analysis_ready_bam/{sample}.dedup.bam",
        bai="analysis_ready_bam/{sample}.dedup.bam.bai",
        ref=config["reference_fasta"]
    output:
        vcf="sv_calls/manta/{sample}/results/variants/diploidSV.vcf.gz"
    params:
        rundir="sv_calls/manta/{sample}"
    conda: "envs/sv_callers.yaml"
    threads: 8
    shell:
        "configManta.py --bam {input.bam} --referenceFasta {input.ref} --runDir {params.rundir} && "
        "python {params.rundir}/runWorkflow.py -m local -j {threads}"

rule delly:
    input:
        bam="analysis_ready_bam/{sample}.dedup.bam",
        bai="analysis_ready_bam/{sample}.dedup.bam.bai",
        ref=config["reference_fasta"]
    output:
        vcf="sv_calls/delly/{sample}.merged.vcf.gz",
        tbi="sv_calls/delly/{sample}.merged.vcf.gz.tbi"
    params:
        prefix="sv_calls/delly/{sample}"
    conda: "envs/sv_callers.yaml"
    shell:
        "delly call -t DEL -g {input.ref} -o {params.prefix}.del.bcf {input.bam} && "
        "delly call -t DUP -g {input.ref} -o {params.prefix}.dup.bcf {input.bam} && "
        "delly call -t INV -g {input.ref} -o {params.prefix}.inv.bcf {input.bam} && "
        "delly call -t TRA -g {input.ref} -o {params.prefix}.tra.bcf {input.bam} && "
        "bcftools concat -a {params.prefix}.*.bcf | bcftools sort -Oz -o {output.vcf} && "
        "tabix -p vcf {output.vcf}"

rule lumpy:
    input:
        bam="analysis_ready_bam/{sample}.dedup.bam",
        bai="analysis_ready_bam/{sample}.dedup.bam.bai"
    output:
        vcf="sv_calls/lumpy/{sample}.vcf"
    params:
        discordants=temp("sv_calls/lumpy/{sample}.discordants.sorted.bam"),
        splitters=temp("sv_calls/lumpy/{sample}.splitters.sorted.bam")
    conda: "envs/sv_callers.yaml"
    threads: 8
    shell:
        "samtools view -b -F 1294 {input.bam} | samtools sort -@ {threads} -o {params.discordants} && "
        "samtools view -h {input.bam} | extractSplitReads_BwaMem -i stdin | samtools view -Sb - | samtools sort -@ {threads} -o {params.splitters} && "
        "lumpyexpress -B {input.bam} -S {params.splitters} -D {params.discordants} -o {output.vcf}"

# --- Stage 4: CNV Calling ---
rule cnvnator:
    input:
        bam="analysis_ready_bam/{sample}.dedup.bam",
        ref_dir=config["reference_chrom_dir"]
    output:
        vcf="cnv_calls/cnvnator/{sample}.vcf"
    params:
        root="cnv_calls/cnvnator/{sample}.root",
        out="cnv_calls/cnvnator/{sample}.cnvnator.out",
        bin_size=1000,
        chroms=" ".join(])
    conda: "envs/cnv_callers.yaml"
    shell:
        "cnvnator -root {params.root} -tree {input.bam} -chrom {params.chroms} && "
        "cnvnator -root {params.root} -his {params.bin_size} -d {input.ref_dir} && "
        "cnvnator -root {params.root} -stat {params.bin_size} && "
        "cnvnator -root {params.root} -partition {params.bin_size} && "
        "cnvnator -root {params.root} -call {params.bin_size} > {params.out} && "
        "cnvnator2VCF.pl {params.out} > {output.vcf}"

# --- Stage 5: Integration and Filtering ---
rule survivor_merge:
    input:
        manta="sv_calls/manta/{sample}/results/variants/diploidSV.vcf.gz",
        delly="sv_calls/delly/{sample}.merged.vcf.gz",
        lumpy="sv_calls/lumpy/{sample}.vcf"
    output:
        vcf=temp("merged_filtered_sv/{sample}.sv.merged.vcf")
    params:
        vcf_list="merged_filtered_sv/{sample}.vcf_list.txt"
    conda: "envs/integration.yaml"
    shell:
        "echo {input.manta} > {params.vcf_list} && "
        "echo {input.delly} >> {params.vcf_list} && "
        "echo {input.lumpy} >> {params.vcf_list} && "
        "SURVIVOR merge {params.vcf_list} 1000 2 1 1 0 50 {output.vcf}"

rule bcftools_filter_sv:
    input:
        vcf="merged_filtered_sv/{sample}.sv.merged.vcf"
    output:
        vcf="merged_filtered_sv/{sample}.sv.merged.filtered.vcf.gz",
        tbi="merged_filtered_sv/{sample}.sv.merged.filtered.vcf.gz.tbi"
    conda: "envs/integration.yaml"
    shell:
        "bcftools filter -i 'SVTYPE!=\"BND\" && QUAL>30 && INFO/PE>5' {input.vcf} | "
        "bgzip -c > {output.vcf} && "
        "tabix -p vcf {output.vcf}"

rule bcftools_filter_cnv:
    input:
        vcf="cnv_calls/cnvnator/{sample}.vcf"
    output:
        vcf="merged_filtered_sv/{sample}.cnv.filtered.vcf.gz",
        tbi="merged_filtered_sv/{sample}.cnv.filtered.vcf.gz.tbi"
    conda: "envs/integration.yaml"
    shell:
        "bcftools filter -i 'INFO/Q0<0.5 && INFO/SVLEN>5000' {input.vcf} | "
        "bgzip -c > {output.vcf} && "
        "tabix -p vcf {output.vcf}"

# --- Stage 6: Annotation ---
rule annotsv:
    input:
        vcf="merged_filtered_sv/{sample}.sv.merged.filtered.vcf.gz"
    output:
        tsv="annotations/{sample}.sv.annotsv.tsv"
    params:
        build="GRCh38"
    conda: "envs/annotation.yaml"
    shell: "AnnotSV -SVinputFile {input.vcf} -outputFile {output.tsv} -genomeBuild {params.build}"

rule vep:
    input:
        vcf="merged_filtered_sv/{sample}.sv.merged.filtered.vcf.gz",
        ref=config["reference_fasta"]
    output:
        vcf="annotations/{sample}.sv.vep.vcf"
    conda: "envs/annotation.yaml"
    threads: 8
    shell:
        "vep --input_file {input.vcf} --output_file {output.vcf} "
        "--vcf --cache --offline --fork {threads} --fasta {input.ref} "
        "--symbol --terms SO --hgvs"
This Snakefile represents a paradigm shift from the bash script. It is not merely an automated version but a qualitatively superior framework. The DAG structure allows Snakemake to understand dependencies, enabling intelligent re-runs and massive parallelization. For a 100-sample cohort, the five independent variant callers per sample result in 500 jobs. A bash script would require a complex, custom-written job submission loop for a cluster. Snakemake handles this natively with a single command (snakemake --cluster...), as demonstrated in workflows like sv-callers.47 The integrated Conda environment management guarantees that the exact versions of all tools are used every time, by anyone, on any system, achieving the gold standard of scientific reproducibility.14Section 12: Configuration, Deployment, and Execution12.1 Configuration FilesThe pipeline is controlled by two simple text files.config.yaml: This file contains global parameters and paths.YAML# config.yaml
# Path to the TSV file listing samples
samples_tsv: "samples.tsv"

# Reference genome and annotation files
reference_fasta: "reference/hg38.fasta"
reference_chrom_dir: "reference/hg38_chrom_dirs" # For CNVnator
adapters_fasta: "adapters/TruSeq3-PE.fa"
samples.tsv: This file lists the samples to be analyzed. The pipeline will generate all results for every sample listed here.Fragment kodu# samples.tsv
sample
NA12878
NA12891
NA12892
12.2 DeploymentDeploying the workflow is straightforward:Install Miniconda: Follow the instructions to install the Miniconda package manager.Create Snakemake Environment: Create a dedicated Conda environment to run Snakemake itself:Bashconda create -n snakemake -c bioconda -c conda-forge snakemake
conda activate snakemake
Clone the Pipeline Repository: Obtain the Snakefile and associated configuration files.Organize Input Data: Place raw FASTQ files, the reference genome, and adapter sequences in the directory structure specified in the config.yaml file.12.3 ExecutionOnce deployed, the workflow can be executed with a few simple commands from the main project directory.Dry-run: Always perform a dry-run first to check the workflow plan and ensure all input files are found correctly.Bashsnakemake -n --use-conda
Visualize the Workflow: Generate a visual representation of the DAG.Bashsnakemake --dag | dot -Tsvg > dag.svg
Local Execution: Run the pipeline on a local machine, using a specified number of cores.Bashsnakemake --cores 8 --use-conda
Cluster Execution (SLURM Example): Submit all jobs to a SLURM compute cluster for massive parallelization. Snakemake handles the submission of each individual job.Bashsnakemake --cluster "sbatch -p partition -c {threads} --mem={resources.mem_mb} -t {resources.time}" \
          --jobs 100 --use-conda
This command tells Snakemake to submit up to 100 jobs concurrently, with each job's resource requests ({threads}, {resources.mem_mb}) being defined within the Snakefile (not shown in the example above, but a best practice for cluster execution).Part IV: Analysis and InterpretationSection 13: Interpreting Pipeline Outputs and Downstream VisualizationThe final outputs of the pipelines are annotated VCF files, which contain a wealth of information. Effective interpretation requires understanding the VCF format and using visualization tools to manually inspect candidate variants.13.1 Deconstructing the Annotated VCFA VCF file has a header section (lines starting with ##) and a data section. For SVs, key information is stored in the INFO column.SVTYPE: The type of structural variant (e.g., DEL, DUP, INV, BND for breakend/translocation).30END: The end coordinate of the variant.SVLEN: The length of the variant. For deletions, this is a negative number.PE, SR: The number of paired-end and split-read supporting reads, respectively. Higher numbers indicate greater confidence.AnnotSV_ACMG_class: The pathogenicity classification from AnnotSV (e.g., pathogenic, likely_benign, VUS).CSQ: The consequence string from VEP, which contains a pipe-delimited list of annotations for each overlapping transcript, including gene symbol, consequence term (e.g., transcript_ablation), and HGVS notation.5013.2 Manual Inspection with IGVNo SV call should be fully trusted without manual visual inspection. The Integrative Genomics Viewer (IGV) is an essential tool for this purpose.51 To validate a candidate variant, one should load the final analysis-ready BAM file (.dedup.bam) and the final annotated VCF file into IGV and navigate to the variant's coordinates.For a Deletion: Look for a region with a visible drop in read coverage. The reads spanning the deletion should have larger-than-expected insert sizes (indicated by a different color in IGV and a high ISIZE value). At the breakpoints, there should be a pileup of split-reads.For a Duplication: Look for an increase in read coverage. Read pairs spanning the duplication will have smaller-than-expected insert sizes.For an Inversion: Look for read pairs that map with an inverted orientation (e.g., forward-forward or reverse-reverse pairs, often colored differently in IGV). The breakpoints will be marked by split-reads.For a Translocation: At the breakpoint, there will be a cluster of reads whose mates map to a different chromosome (inter-chromosomal pairs).30Visual inspection provides an indispensable sanity check and is the ultimate arbiter of a variant's validity, especially for complex or borderline calls.Section 14: A Comparative Analysis and Strategic Recommendations14.1 Bash vs. Snakemake: A Critical ComparisonBoth the bash script and the Snakemake workflow implement the same analytical logic, but they represent fundamentally different approaches to pipeline construction with distinct advantages and disadvantages.FeatureSequential Bash ScriptSnakemake WorkflowEase of UseHigh for single execution. Simple to run (./script.sh).Moderate learning curve. Requires understanding of rules and configuration.ScalabilityPoor. Does not scale to multiple samples or clusters without significant modification (e.g., custom loops, job submission logic).Excellent. Natively scales from a single core to thousands of cluster nodes with a single command.ReproducibilityPoor. Relies on the user having the correct software versions installed globally. Results can vary between systems.Excellent. Integrated Conda environment management guarantees that the exact software versions are used for every step, ensuring full computational reproducibility.ModifiabilityModerate. Adding a new step requires careful modification of the linear script.High. Highly modular. New rules can be added without altering existing ones. The DAG is automatically updated.Error RecoveryNone. If a step fails, the script halts. The user must manually determine where to restart.Excellent. Automatically resumes from the point of failure, re-running only the necessary jobs.14.2 Strategic RecommendationsThe choice of which pipeline to use should be guided by the specific needs of the project.The Bash Script is recommended for:Pedagogical Purposes: It serves as an excellent tool for learning the command-line syntax and data flow of an SV/CNV analysis.Single-Sample Analysis: For a quick analysis of one or two samples on a local machine, the bash script is straightforward and effective.Constrained Environments: In situations where installing a workflow manager like Snakemake is not possible, the script provides a self-contained solution.The Snakemake Pipeline is the recommended choice for:All Multi-Sample Projects: Its ability to automatically parallelize and manage jobs across many samples makes it indispensable for any cohort-scale analysis.Published Research: The guarantee of reproducibility provided by the integrated environment management is a critical requirement for modern scientific research.HPC Environments: It is designed for seamless deployment on high-performance computing clusters, making it the standard for large-scale genomic data analysis.In summary, while the bash script is a valuable instructional and utility tool, the Snakemake pipeline represents the robust, scalable, and reproducible framework required for production-level genomics research.AppendixAppendix A: Environment and Tool InstallationThe recommended method for installing all required software is via Miniconda and the Bioconda channel. The Snakemake pipeline will handle tool installation automatically via its conda directives. For the bash script, the environments must be created manually.Example environment.yml for the bash script:YAMLname: sv-cnv-pipeline
channels:
  - bioconda
  - conda-forge
  - defaults
dependencies:
  - fastqc
  - multiqc
  - trimmomatic
  - bwa
  - samtools
  - picard
  - manta
  - delly2
  - lumpy-sv
  - cnvnator
  - gatk4
  - survivor
  - bcftools
  - annotsv
  - ensembl-vep
Installation Command:Bashconda env create -f environment.yml
conda activate sv-cnv-pipeline
Appendix B: Full Code ListingsThe complete, uninterrupted source code for both the sv_cnv_pipeline.sh script and the Snakemake workflow (Snakefile, config.yaml, samples.tsv) are provided in Sections 9 and 11-12, respectively.Appendix C: Key Pipeline Tool ParametersThis table provides a reference for the most critical parameters that may need to be tuned depending on the specific dataset and research question.Tool/StageParameterRecommended ValueDescriptionTrimmomaticSLIDINGWINDOW4:20Window size and average quality threshold for trimming. Lower quality threshold for noisier data.TrimmomaticMINLEN50Minimum read length to keep after trimming. Increase for higher quality data.BWA-MEM-M(flag)Marks shorter split-hits as secondary. Essential for Picard compatibility.CNVnatorbin_size1000 (for 30x WGS)Analysis bin size. Decrease for higher coverage (e.g., 500 for 60x) for better resolution; increase for lower coverage.13GATK gCNVbin-length1000 (for 30x WGS)Similar to CNVnator's bin size; defines the resolution of the analysis.13SURVIVORmin_supporting_callers2Minimum number of callers required to support a merged SV. 1 for high sensitivity, 3 for high specificity.37BCFtoolsfilter -i 'QUAL>X'X=30Minimum quality score. A starting point; should be evaluated based on callset characteristics.BCFtoolsfilter -i 'INFO/PE>X'X=5Minimum number of supporting paired-end reads. A powerful filter for SVs.AnnotSV-benignAF0.01Allele frequency threshold in population databases to classify an SV as benign.41 Lower for stricter rare variant analysis.